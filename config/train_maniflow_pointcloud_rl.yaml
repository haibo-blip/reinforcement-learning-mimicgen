# ManiFlow RL Training Configuration
# Complete configuration for PPO fine-tuning of ManiFlow policies

defaults:
  - _self_

# Weights & Biases logging
use_wandb: true
wandb_project: "maniflow_rl"
wandb_run_name: "ppo_finetuning_${task.env_runner.env_meta.env_name}"
wandb_tags: ["maniflow", "ppo", "robotics", "rl"]

# Output directory
output_dir: "./outputs/maniflow_rl_${now:%Y-%m-%d_%H-%M-%S}"

# Policy configuration
policy:
  _target_: equi_diffpo.policy.maniflow.maniflow_pointcloud_rl_policy.ManiFlowRLPointcloudPolicy

  # Model architecture
  horizon: 16
  n_action_steps: 8
  n_obs_steps: 2

  # Diffusion parameters
  noise_method: "flow_sde"
  num_inference_steps: 10
  noise_level: 0.5
  noise_anneal: true

  # Transformer parameters
  n_layer: 4
  n_head: 8
  n_emb: 256
  encoder_output_dim: 128
  visual_cond_len: 256

  # RL-specific
  add_value_head: true
  joint_logprob: false  # Single timestep vs joint estimation (like Pi0.5)

  # Pretrained checkpoint (optional)
  checkpoint: null  # Path to pretrained policy checkpoint

# Environment configuration
task:
  # Dataset for shape metadata and normalizer
  dataset_path: null  # Path to HDF5 dataset

  env_runner:
    _target_: equi_diffpo.env_runner.robomimic_rl_runner.RobomimicRLRunner
    collect_rl_data: true

    # Environment metadata
    env_meta:
      env_name: "Lift"  # Environment name for logging

    # Robomimic environment configuration
    env_config:
      env_name: "Lift"
      robots: "Panda"
      controller_configs:
        type: "OSC_POSE"
        interpolation: "linear"
        ramp_ratio: 0.2
      gripper_types: "default"
      initialization_noise: 0.02
      table_full_size: [0.8, 0.8, 0.05]
      table_friction: [1.0, 0.005, 0.0001]
      use_object_obs: true
      use_camera_obs: true
      camera_names: ["agentview", "robot0_eye_in_hand"]
      camera_height: 84
      camera_width: 84
      reward_shaping: true

    # Environment runner parameters
    max_steps: 400  # Max steps per episode
    num_episodes: 100  # Episodes per rollout
    render_video: true
    video_skip: 5
    video_path: null  # Auto-generated

    # Action parameters
    abs_action: true
    past_action: true

# Training configuration
training:
  # PPO parameters
  total_timesteps: 1000000
  num_envs: 8
  num_steps_per_rollout: 256  # Steps per rollout per environment
  batch_size: 512             # Minibatch size for training
  num_epochs: 4               # PPO epochs per rollout
  max_grad_norm: 0.5          # Gradient clipping

  # PPO hyperparameters
  clip_range: 0.2             # PPO clip parameter
  entropy_coef: 0.01          # Entropy loss coefficient
  value_coef: 0.5             # Value loss coefficient
  target_kl: 0.01             # Target KL divergence for early stopping

  # Learning rates
  learning_rate: 3e-4         # Adam learning rate
  lr_schedule: "linear"       # linear, constant, cosine
  warmup_steps: 10000         # LR warmup steps

  # Environment parameters
  max_episode_length: 400
  action_chunk_size: 8
  obs_chunk_size: 2

  # Logging and saving
  log_interval: 10            # Log every N rollouts
  save_interval: 100          # Save every N rollouts
  eval_interval: 50           # Eval every N rollouts

# Advantage computation (GAE)
advantage:
  gamma: 0.99                 # Discount factor
  gae_lambda: 0.95           # GAE lambda parameter
  normalize_advantages: true  # Normalize advantages
  normalize_returns: false   # Normalize returns

# Shape metadata (will be loaded from dataset or inferred)
shape_meta:
  obs:
    robot0_eye_in_hand_image:
      shape: [3, 84, 84]
      type: rgb
    point_cloud:
      shape: [1024, 6]
      type: point_cloud
    robot0_eef_pos:
      shape: [3]
    robot0_eef_quat:
      shape: [4]
    robot0_gripper_qpos:
      shape: [2]
  action:
    shape: [10]

# Hydra configuration
hydra:
  run:
    dir: ${output_dir}
  job:
    chdir: true